thetas_states_df$R0[2] = "free"
thetas_states_df$R0[3] = "free"
thetas_states_df$R0[4] = "free"
thetas_states_df$R0[5] = "free"
thetas_states_df
# Run the params_to_likelihood_v1() function once, to see your starting likelihood
data.points = d3$active_cases
params = thetas_states_table[thetas_states_df=="free"] # starting parameter values
params_to_likelihood_v1(params, data.points, thetas_states_df, delay_val=0, printvals=TRUE, makePlots=FALSE)
# Running the Maximum Likelihood search with the optim() function.
# LOOK AT THE OUTPUT THAT PRINTS TO SCREEN!!
ML_results = optim(par=params, fn=params_to_likelihood_v1, data.points=data.points, thetas_states_df=thetas_states_df, delay_val=0, printvals=TRUE, method="L-BFGS-B", lower=0.0, control=list(fnscale=-1))
# Graphing the ML model
# Take the learned parameters from "ML_results", put them
# into a theta_states data.frame for simulation and plotting
thetas_states_ML = thetas_states_df
TF = thetas_states_ML == "free"
thetas_states_ML[TF] = ML_results$par
for (i in 1:ncol(thetas_states_ML))
{ thetas_states_ML[,i] = as.numeric(thetas_states_ML[,i]) }
thetas_states_ML$time[2:length(thetas_states_ML$time)] = thetas_states_ML$time[2:length(thetas_states_ML$time)]
# Plot the results
trajectory = simulate_SIR_changes(thetas_states_df=thetas_states_ML, times=times)
maxy = max(max(trajectory$I), max(data.points))
xvals = c(trajectory$time, times)
yvals = c(trajectory$I, data.points)
plot(x=xvals, y=yvals, pch=".", col="white", xlim=c(0, max(trajectory$time)), ylim=c(0, maxy), xlab="Day", ylab="Number of individuals")
lines(x=trajectory$time, y=trajectory$I, lwd=3, col="firebrick2")
points(times, data.points, col="red", pch="+")
legend(x="topleft", legend=c("Active COVID-19 case count", 'ML-fitted projection of "I" (Infected)'), lty=c("blank", "solid"), lwd=c(1,3), pch=c("+", "."), col=c("red","firebrick2"), cex=0.8)
titletxt = paste0("ML fit, active cases from: ", country_name, "\nM5 (a 5-regime, 5 param model) max lnL = ", round(ML_results$value, 2))
title(titletxt)
# Save this model's parameters and log-likelihood
thetas_states_ML_model5 = thetas_states_ML
total_lnL_Model5 = ML_results$value
#######################################################
# Model M6: A 6-regime model, 6 free parameters
#######################################################
times = 1:nrow(d3)      # Number of days since first day
# Set up regimes:
# NZ Level 4 lockdown went in on March 26:
# March 26 = Day 87, +1 for 31/21/2019, +8 for delay to see lockdown effect = 96
# Allowing an earlier pre-lockdown slowdown due to social distancing & public health = 85
# Allowing an earlier pre-lockdown slowdown due to social distancing & public health = 85
# Allowing an uptick in July (in real life, these are imported cases) = 140
case1 = (1:nrow(d3))[d3$active_cases>0][1] # day of the first detected case
time = c(1, case1, 85, 94, 96, 140)
R0 = c(0.0, 3.0, 1.5, 1.5, 0.3, 1.5)    # initial guesses
D_inf = c(8.0, 8.0, 8.0, 8.0, 8.0, 8.0) # seems to fit data
S = c(as.numeric(d3$population[1]), 0, 0, 0, 0, 0) # putting in the country's population size
I = c(0, 1, 0, 0, 0, 0) # the number of cases on day 1 is a "nuisance parameter"
R = c(0, 0, 0, 0, 0, 0) # no vaccinations
thetas_states_table = cbind(time, R0, D_inf, S, I, R)
thetas_states_df = as.data.frame(thetas_states_table, stringsAsFactors=FALSE)
thetas_states_df
# Make some parameters into "free" parameters, to be inferred
thetas_states_df$I[2] = "free"
thetas_states_df$R0[2] = "free"
thetas_states_df$R0[3] = "free"
thetas_states_df$R0[4] = "free"
thetas_states_df$R0[5] = "free"
thetas_states_df$R0[6] = "free"
thetas_states_df
# Run the params_to_likelihood_v1() function once, to see your starting likelihood
data.points = d3$active_cases
params = thetas_states_table[thetas_states_df=="free"] # starting parameter values
params_to_likelihood_v1(params, data.points, thetas_states_df, delay_val=0, printvals=TRUE, makePlots=FALSE)
# Running the Maximum Likelihood search with the optim() function.
# LOOK AT THE OUTPUT THAT PRINTS TO SCREEN!!
ML_results = optim(par=params, fn=params_to_likelihood_v1, data.points=data.points, thetas_states_df=thetas_states_df, delay_val=0, printvals=TRUE, method="L-BFGS-B", lower=0.0, control=list(fnscale=-1))
# Graphing the ML model
# Take the learned parameters from "ML_results", put them
# into a theta_states data.frame for simulation and plotting
thetas_states_ML = thetas_states_df
TF = thetas_states_ML == "free"
thetas_states_ML[TF] = ML_results$par
for (i in 1:ncol(thetas_states_ML))
{ thetas_states_ML[,i] = as.numeric(thetas_states_ML[,i]) }
thetas_states_ML$time[2:length(thetas_states_ML$time)] = thetas_states_ML$time[2:length(thetas_states_ML$time)]
# Plot the results
trajectory = simulate_SIR_changes(thetas_states_df=thetas_states_ML, times=times)
maxy = max(max(trajectory$I), max(data.points))
xvals = c(trajectory$time, times)
yvals = c(trajectory$I, data.points)
plot(x=xvals, y=yvals, pch=".", col="white", xlim=c(0, max(trajectory$time)), ylim=c(0, maxy), xlab="Day", ylab="Number of individuals")
lines(x=trajectory$time, y=trajectory$I, lwd=3, col="firebrick2")
points(times, data.points, col="red", pch="+")
legend(x="topleft", legend=c("Active COVID-19 case count", 'ML-fitted projection of "I" (Infected)'), lty=c("blank", "solid"), lwd=c(1,3), pch=c("+", "."), col=c("red","firebrick2"), cex=0.8)
titletxt = paste0("ML fit, active cases from: ", country_name, "\nM6 (a 6-regime, 6 param model) max lnL = ", round(ML_results$value, 2))
title(titletxt)
# Save this model's parameters and log-likelihood
thetas_states_ML_model6 = thetas_states_ML
total_lnL_Model6 = ML_results$value
# Calculate AIC and AIC model weights by editing the example Google Sheet files:
#
# Example spreadsheet calculating AIC and AIC model weights:
#
# (You will have to File->Make A Copy, or copy-paste to your own Google Sheet, this one is edit-locked.)
# https://docs.google.com/spreadsheets/d/13xJNrnMVGJFAp3MWN5t4bzRKVXQzsY0yeYcs2U14Xak/edit?usp=sharing
#
# Type in the log-likelihoods, and inferred parameters, for your models. For the
# examples above, this will make R print them to screen.
# Print the log-likelihoods to screen
cat(c(total_lnL_Model1, total_lnL_Model2, total_lnL_Model3, total_lnL_Model4, total_lnL_Model5, total_lnL_Model6), sep="\n")
# Print the I_ini to screen
cat(c(thetas_states_ML_model1$I[2], thetas_states_ML_model2$I[2], thetas_states_ML_model3$I[2], thetas_states_ML_model4$I[2], thetas_states_ML_model5$I[2], thetas_states_ML_model6$I[2]), sep="\n")
# Print the R0 to screen
thetas_states_ML_model1$R0
thetas_states_ML_model2$R0
thetas_states_ML_model3$R0
thetas_states_ML_model4$R0
thetas_states_ML_model5$R0
thetas_states_ML_model6$R0
#######################################################
# Congratulations! You have just done basic statistical
# model comparison.
# To understand the basics of what AIC is about, please
# see the lectures, and read the notes below.
#######################################################
#######################################################
# Statistical model comparison
#
# One huge advantage of likelihood-based inference is
# that the likelihoods can be used to statistically
# compare model fits.
#
# One very common method for comparing ML-fitted models
# is Akaike Information Criterion (AIC). AIC has been
# used in tens of thousands of research papers to
# compare multiple model fits. See the lecture slides/
# audio, and the Week 4 reading, for an introduction to
# AIC.
#
# The very-very short version of AIC is that AIC
# *estimates the relative distance from the truth*.
#
# We never know what the "true model" is in real life,
# since any true biological history that produced observations
# is preposterously complex.  However, we can at least
# compare the *relative* fit of models to data, using AIC.
#
# AIC is a simple function of likelihood:
#
# AIC = -2 * (lnL - nparams)
#
# lnL = the maximized log-likelihood under some model
# nparams = the number of free parameters that you fit with ML
#
# Basically, the AIC is a form of penalized likelihood: we
# favor models that give higher lnLs, but then we penalize
# models base on the number of free parameters that we fit.
#
# This helps enforce parsimony in our explanations. If we
# put no penalty on extra free parameters, we could always
# fit models with higher and higher likelihood, until we
# ended up with a free parameter for every data point, at
# which point we fit all the data perfectly, but we have
# explained nothing at all, because the model just restates
# the dataset.
#
# Some details:
#
# The "-2" in the AIC formula needs explanation. The "-"
# just converts the log-likelihood (which is always negative,
# if the data are discrete) from negative to positive. This
# makes sense because AIC is a distance, and distances
# should be positive.
#
# The "2" part in AIC is actually an arbitrary convention -
# it would work the same if we used 1, 3, etc. The number "2"
# was chosen because another famous statistical test, the
# Likelihood Ratio Test, uses 2*(difference in log-likelihoods)
# as the test statistic, often called the "deviance".
#
# The rationale for 1 parameter = 1 unit of log-likelihood
# is complex, but it can be derived from a view of the world
# where we think that processes are controlled by a series of
# parameters of decreasing importance, such that there are
# a few major controlling variables, and an exponential
# decay in the importance of other controlling variables.
#
# Other rationales can produce other penalties, creating
# other model comparison criteria, such as BIC (Bayesian
# Information Criterion). Don't worry about these for now.
#
# AIC model weights
#
# AIC is just a number that doesn't mean much by itself.
# AIC values are used to *compare* multiple models. We
# do this with AIC weights, which assign each model a
# weight, where all of the weights sum to 1.
#
# For example, if you had these weights:
#
# Model 1 AIC weight = 0.05
# Model 2 AIC weight = 0.9
# Model 3 AIC weight = 0.05
#
# You would say that "Model 2 acquired 90% of the AIC
# weight, and was the best-fitting model in our
# model set. It fits about 9 times better than the
# other models combined, which is moderately good
# evidence that it is a better fit. Models 1 and 3
# have equal model weight, so they are approximately
# equivalent fits to the data."
#
##############################################
# Here is how to calculate AIC weights:
##############################################
#
# 1. Calculate AIC values for all the models
#    in your model set.
#
# 2. Find the AIC-best model. This is the model
#    with the *LOWEST* AIC (i.e., the smallest
#    relative distance from the truth).
#
# 3. For all the models, calculate the AIC difference
#    from the best model. (The AIC difference between
#    the best model and itself is 0.0.).  This
#    number is called "deltaAIC" (in science, "delta"
#    often means "change" or "difference", so "deltaAIC"
#    just means "difference in AIC".
#
# 4. For each model, calculate exp(-0.5 * deltaAIC). These
#    are the relative likelihoods.
#
# 5. Sum the relative likelihoods, and divide each
#    individual relative likelihood by the sum of
#    relative likelihoods. This is the AIC weight.
#    The AIC weights will sum to 1.0.
#
# 6. If desired, convert these fractions (numbers
#    between 0 and 1) to percentages, by multiplying
#    by 100, or by clicking the "%" option in Excel.
#
###############################################
###############################################
# Advantages and disadvantages of AIC
###############################################
# The advantages of the AIC framework include:
#
# * Multiple models (more than 2) can be compared easily.
#   Traditional statistical tests, which give you p-values,
#   typically only compare two models at a time, namely
#   a null model and an alternate.
#
# * There is no concept of "p-value cutoffs" or
#   "statistical signficance" in AIC-based analyses.
#   The proponents of AIC acknowledge that evidence is
#   continuous. If AIC favours a model 1 over model 2
#   60%-40%, this is a tiny bit of evidence that the
#   model 1 is closer to the truth.  If AIC favours a
#   model 1 99.999% to 0.001%, then this is strong
#   evidence that model 1 closer to the truth.
#
# * AIC assumes that we never have access to the
#   complete, true model that produced our data. It
#   acknowledges that the "true" model behind most
#   empirical data would be fantastically complex. So
#   all we are really trying to do is find better
#   simple approximations of a very complex reality.
#   AIC is one easy way to measure the *relative*
#   distance of models from the truth. We never know
#   the "absolute" distance from the truth, because
#   that would require knowing the true model, which
#   we never do.
#
# * Assuming you have maximized lnL values for each
#   model, the formula for AIC, and AIC weights, is very
#   simple.  The calculations can be done by hand, or
#   in Excel or Google Sheets (or R). All you have to
#   be able to do is count the number of free parameters
#   in each model.
#
# * AIC can be used very broadly for comparing models.
#   This includes not just models where the lnL is
#   explicitly reported (like those above), but
#   also in linear regression and other linear models.
#   Here, model fit is more commonly reported in
#   terms of "deviance" or "RSS" (residual sum
#   of squares, the sum of the squared errors between
#   the line of best fit, and the data).  These are
#   all proportional to -1 * log-likelihood, under
#   the assumption that the errors are normally
#   distributed. So, the "least squares" method can
#   also be interpret as a Maximum Likelihood method!
#
#   (Unfortunately, many introductory students only
#    ever learn about least-squares, when Maximum
#    Likelihood has much broader application to
#    all sorts of models, not just cases where
#    a normal distribution of errors can be assumed.)
#
# AIC has some limitations as well:
#
# * If you are literally doing a randomized controlled
#   experiment, where you have two hypotheses (models)
#   that exhaust the space of models, namely "no effect"
#   (the null model) and "effect" (the alternative model),
#   then the traditional p-value framework can make a
#   lot of sense. This is especially the case where
#   further decisions will be based on the result, so
#   the effect of false-positives and false-negatives
#   needs serious consideration.
#
# * If you *do* have the true model in your set, for
#   example when your data come from a computer simulation
#   where you know the true model, then a criterion
#   like BIC, which has a stronger penalty against
#   extra parameters.
#
# * If you have a dataset where the number of data is
#   small (typically <30 data points), and/or the number
#   of free parameters is large, it is more appropriate
#   to use "AICc", which is sample-size corrected AIC.
#   The formula for this is a little more complex, so
#   we will ignore it here.
#
# * There are all kinds of intense philosophical debates
#   in statistics and philosophy of science about
#   models, inference, and the best way to measure
#   support for models/hypotheses. AIC is just one
#   choice amongst these. I would put it roughly in
#   the "likelihoodist" school. Other major schools of
#   thought are "frequentism" (which includes p-values
#   and worries about the long-term error rate) and
#   Bayesianism (which explicitly puts prior probability
#   distributions on parameter values, and on models.)
#
# * It is easy forget that "all models are wrong", and
#   get very attached to your best model.  This is a
#   challenge in all of statistics & data analysis,
#   however.  The famous statement from George Box is
#   "All models are wrong, but some models are useful."
#   Constant critical thinking is required, as I have
#   tried to encourage!
#######################################################
#######################################################
# What is the point of all of this?
#
# In biology, we almost never know the "true model". We
# are just trying to find better approximations of
# reality.  AIC is one common choice to balance fitting
# the observations against parsimony.
#
# AIC can be used to address common questions like
# "which model is the best, given the observations I
#  have", or "out of this set of models, which models
#  are reasonable, given the dataset"?
#
# AIC can be used any time you can get a maximized
# log-likelihood, and a count of the number of free
# parameters, on each model.  It is used so often that
# R has some standard functions to provide them for
# linear models provided by the lm() function.
# Note: these functions will not work
# outside of standard "base" R functions like lm() and
# glm().
#
# Here is a quick example of that:
data("iris")  # A default R dataset, measurements of iris flowers
plot(iris$Sepal.Length, iris$Petal.Length)
# Run a simple linear model (lm), with Sepal Length
# predicting Petal Length
lm_result = lm(iris$Petal.Length~Sepal.Length, data=iris)
# The "logLik" function reports the lnL, and the
# number of free parameters (df=3)
# (df means "degrees of freedom", i.e. number of free
#  parameters)
logLik(lm_result)
AIC(lm_result)
# You can see that the AIC formula is being used:
lnL = as.numeric(logLik(lm_result))
-2*(lnL - 3)
AIC(lm_result)
#
# MAIN POINT: The skills you have used for using
# Maximum Likelihood and AIC on SIR models can be
# used anywhere from quite complex process-based
# models used in epidemiology, ecology, phylogenetics
# etc., all the way to the basic linear models used
# in introductory statistics.
#
#######################################################
################################################
# Appendix: The connection between Ordinary
# Least Squares (OLS), i.e. linear regression,
# and Maximum Likelihood.
#
# This is something I wish I had been taught
# early in my statistics education, so I am
# mentioning it here.
#
# This Appendix material is "bonus", I will not
# examine you on it for my section of the course.
#
################################################
#
# In standard "least squares" analyses
# (OLS, Ordinary Least Squares), where
# we talk about the line of
# best fit being the one that minimizes
# the sum of squared residuals (or RSS, residual
# sum of squares), this is just another way
# of talking about maximizing the log-likelihood,
# *if* we make the assumption that the residuals
# (the difference between the predicted line and
#  the observed response) all follow the same
# normally distribution (in other words, the
# residuals are independent, normally distributed
# with constant variance).
#
# In this situation, the log-likelihood for the
# line of best fit is derived from the log of
# the probability density function for a normal
# distribution.
#
# I find that this is rarely explained in R code,
# so I am putting various formulations here.
#
# Here is the logLik() function code:
getAnywhere("logLik.lm")
# This is the formula from logLik, with weights (w)
# set to 1s (meaning equal weighting of all residuals):
0.5 * - N * (log(2 * pi) + 1 - log(N) +
log(sum(lm_result$residuals^2)))
# Here are several simpler formulations that
# give the same result:
#
# Formula for the log-likelihood of a linear model
# with x predicting y
# Source:
# https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/06/lecture-06.pdf
# ...page 2, equation (3)
#
# N = number of observations
#
# RSS = residual sum of squares =
# sum of (yi - (b0 + b1*xi))^2
# where
# yi = ith response (observation)
# b0 = intercept
# b1 = slope
# xi = ith predictor
#
# stdev_squared = variance = RSS / N
# stdev = standard deviation = sqrt(stdev_squared) = sqrt(variance)
N = length(lm_result$residuals)
RSS = sum(lm_result$residuals^2)
stdev_squared = RSS/N
stdev = sqrt(RSS/N)
-N/2 * log(2*pi) - N*log(stdev) - 1/(2*stdev_squared)*RSS
# The formula works the same if you just use RSS
-N/2 * log(2*pi) - N*log(sqrt(RSS/N)) - 1/(2*RSS/N)*RSS
# R's logLik() function has the above formula re-arranged
# so that RSS appears only once:
0.5 * - N * (log(2 * pi) + 1 - log(N) +
log(RSS))
logLik(lm_result)
# These formulations all give the same result!
#######################################################
# References
#
# Cosma Shalizi (2015). "Lecture 6: The Method of
# Maximum Likelihood for Simple Linear Regression."
# Lecture 6 from Modern Regression, 36-401, Fall 2015.
# https://www.stat.cmu.edu/~cshalizi/mreg/15/lectures/06/lecture-06.pdf
#
#######################################################
#######################################################
# References
#
# My SIR code relies heavily on code from the fitR package
# (Funk et al. 2019).
#
# Adam, David (2020). "UK has enough intensive care units for
# coronavirus, expert predicts." New Scientist, March 25, 2020.
# https://www-newscientist-com.ezproxy.auckland.ac.nz/article/2238578-uk-has-enough-intensive-care-units-for-coronavirus-expert-predicts/
#
# Anderson, Charles (2020). "New Zealand coronavirus deaths during lockdown could
# be just 20, modelling suggests." The Guardian. 27 Mar 2020.
# https://www.theguardian.com/world/2020/mar/27/new-zealand-coronavirus-deaths-during-lockdown-could-be-just-20-modelling-suggests
#
# Dillane, Tom; Knox, Chris (2020), "Coronavirus Covid 19: New Zealand's
# best and worst prepared DHBs by elderly population and ICU beds."
# NZ Herald. April 5, 2020
# https://www.nzherald.co.nz/nz/news/article.cfm?c_id=1&objectid=12321390
#
# Fefferman, Nina (2020). "The role of applied math in real-time pandemic
# response: How basic disease models work." Date: 3:30 EDT Tuesday,
# March 31, 2020.  http://www.nimbios.org/webinars#fefferman 
#
# Funk, Sebastian (2019). "Introduction to model fitting in R." http://sbfnk.github.io/mfiidd/introduction.html
#
# Funk, Sebastian; Camacho, Anton; Johnson, Helen; Minter, Amanda; O’Reilly, Kathleen; Davies, Nicholas (2019). "Model fitting and inference for infectious disease dynamics."  Centre for the Mathematical Modelling of Infectious Diseases (CMMID), London School of Hygiene & Tropical Medicine.
#
# James, Alex; Hendy, Shaun C.; Plank, Michael J.; Steyn, Nicholas (2020). "Suppression and Mitigation
# Strategies for Control of COVID-19 in New Zealand." 25 March 2020.
# https://cpb-ap-se2.wpmucdn.com/blogs.auckland.ac.nz/dist/d/75/files/2017/01/Supression-and-Mitigation-New-Zealand-TPM-006.pdf
#
# Newton, Kate (2020). "The man modelling NZ's Covid-19 spread
# from his kitchen table." Radio New Zealand. 27 March 2020.
# https://www.rnz.co.nz/news/in-depth/412744/the-man-modelling-nz-s-covid-19-spread-from-his-kitchen-table
#########################################################
View(thetas_states_ML_model4)
View(thetas_states_ML_model3)
View(thetas_states_ML_model2)
View(thetas_states_ML_model1)
View(thetas_states_ML_model4)
View(thetas_states_ML_model5)
View(thetas_states_ML_model6)
